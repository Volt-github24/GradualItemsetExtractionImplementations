{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w3QwHGtFCnk"
      },
      "source": [
        "# Implémentation complète en Apache Spark de l'algorithme proposé à l'aide de PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9nMAOs6FCnu",
        "outputId": "ec5bab54-0e5e-408f-eb7a-c4540a3770dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=43de07ec7f43273d8de3ff2a94c1407a720adfb1e54e0b8074caf1afdc4983ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import itertools\n",
        "#import os\n",
        "#import shutil\n",
        "#import sys\n",
        "#import argparse\n",
        "\n",
        "from scipy.sparse import csr_matrix as csr\n",
        "\n",
        "\n",
        "# Selon la configuration de Spark, les lignes suivantes peuvent être nécessaires.\n",
        "!pip install pyspark\n",
        "import pyspark as spark\n",
        "from pyspark.sql import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfVKONXwFCny"
      },
      "source": [
        "## Fonctions requises pour calculer le coefficient de corrélation de rang flou."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FRSghm1vFCn0"
      },
      "outputs": [],
      "source": [
        "# Norme T de Lukasiewicz.\n",
        "def T_norm(x,y):\n",
        "    return max(0,x+y-1)\n",
        "\n",
        "# TL-E-Commande fortement complète dans R.  \n",
        "def Fuzzy_ordering(x,y,r):\n",
        "    return min(1,max(0,1-((x-y)/r)))\n",
        "\n",
        "# Relation floue stricte (Rx ou Ry)\n",
        "def Fuzzy_relation(x1,x2,r):\n",
        "    return 1-Fuzzy_ordering(x2,x1,r)\n",
        "\n",
        "# Le degré d'accord entre deux paires étant donné un r sera alors calculé comme suit.\n",
        "def Concordance_degree(pair1,pair2,r):\n",
        "    return T_norm(Fuzzy_relation(pair1[0], pair1[1],r), Fuzzy_relation(pair2[0], pair2[1],r))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSJec7CeFCn1"
      },
      "source": [
        "## Génération de la matrice d'adjacence\n",
        "Cette fonction renvoie la matrice des degrés d'accord de l'ensemble des motifs de taille k = 2 qui lui est transmise en tant qu'argument, ainsi qu'un r et l'ensemble de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "MxXtWpWTFCn9",
        "outputId": "05859a0e-36b4-4212-935e-00bfcf7573d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    Nous évaluerons ces autres combinaisons puisqu'il n'y aura que des itemsets de taille 2 du\\n    type (('atributo1', '>'), ('atributo2', '>')) et (('atributo1', '>'), ('atributo2', '<'))\\n\\n    if op1 == '<' and op2 == '>': \\n        for i in range(N):\\n            for j in range(N):\\n                if i != j:\\n                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[i], at2[j]],r)          \\n    \\n    \\n\\n    if op1 == '<' and op2 == '<': \\n        for i in range(N):\\n            for j in range(N):\\n                if i != j:\\n                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[j], at2[i]],r)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Il utilise un flotteur 16 bits au lieu du flotteur python standard qui utilise 64 bits.\n",
        "def generate_matrix(candidate, df, r):\n",
        "    \n",
        "    at1 = pd.to_numeric(df.value[candidate[0][0]])\n",
        "    op1 = candidate[0][1]\n",
        "    \n",
        "    at2 = pd.to_numeric(df.value[candidate[1][0]])\n",
        "    op2 = candidate[1][1]\n",
        "    \n",
        "    N = len(at1)     # N est le nombre d'attributs\n",
        "    matrix = np.zeros((N, N), dtype = np.float16)\n",
        "    \n",
        "    if op1 == '>' and op2 == '>': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = Concordance_degree([at1[i], at1[j]],[at2[i], at2[j]],r)\n",
        "                    \n",
        "    if op1 == '>' and op2 == '<': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = Concordance_degree([at1[i], at1[j]],[at2[j], at2[i]],r)\n",
        "                    \n",
        "    return (tuple(candidate), csr(matrix))\n",
        "                \n",
        "\"\"\"\n",
        "    Nous évaluerons ces autres combinaisons puisqu'il n'y aura que des itemsets de taille 2 du\n",
        "    type (('atributo1', '>'), ('atributo2', '>')) et (('atributo1', '>'), ('atributo2', '<'))\n",
        "\n",
        "    if op1 == '<' and op2 == '>': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[i], at2[j]],r)          \n",
        "    \n",
        "    \n",
        "\n",
        "    if op1 == '<' and op2 == '<': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = Concordance_degree([at1[j], at1[i]],[at2[j], at2[i]],r)\n",
        "\"\"\"                 \n",
        "    \n",
        "\n",
        "\n",
        "                    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ept3SqYrFCoC"
      },
      "source": [
        "## Génération de la matrice de d'adjacence. 8 bits\n",
        "Variante de la fonction précédente, pour le cas où une précision de 8 bits est choisie, des entiers de 8 bits sont utilisés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SVEm4AM4FCoF"
      },
      "outputs": [],
      "source": [
        "# Utilisez des entiers non signés 8 bits au lieu du flotteur python standard qui utilise 64 bits.\n",
        "def generate_matrix_int(candidate, df, r):\n",
        "    \n",
        "    at1 = pd.to_numeric(df.value[candidate[0][0]])\n",
        "    op1 = candidate[0][1]\n",
        "    \n",
        "    at2 = pd.to_numeric(df.value[candidate[1][0]])\n",
        "    op2 = candidate[1][1]\n",
        "    \n",
        "    N = len(at1)     # N est le nombre d'attributs\n",
        "    matrix = np.zeros((N, N), dtype = np.uint)\n",
        "    \n",
        "    if op1 == '>' and op2 == '>': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = (255*Concordance_degree([at1[i], at1[j]],[at2[i], at2[j]],r)) \n",
        "                    \n",
        "    if op1 == '>' and op2 == '<': \n",
        "        for i in range(N):\n",
        "            for j in range(N):\n",
        "                if i != j:\n",
        "                    matrix[i,j] = (255*Concordance_degree([at1[i], at1[j]],[at2[j], at2[i]],r))\n",
        "                    \n",
        "    return (tuple(candidate), csr(matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIXVD7oKFCoH"
      },
      "source": [
        "\n",
        "##Calcul du support\n",
        "Renvoie le support de tableau qui lui est passé en argument (qui correspond à la matrice d'adjacence du motif) sous la forme de la somme de tous ses éléments divisée par N(N-1)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AN1FY8hdFCoK"
      },
      "outputs": [],
      "source": [
        "# Renvoie le support de la matrice\n",
        "def compute_support(m):\n",
        "    N = m.shape[1]\n",
        "    return csr.sum(m)/(N*((N-1)/2))   # Il est également possible d'utiliser directement np.sum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6vOwibxFCoM"
      },
      "source": [
        "## calcul du support 8 bits.\n",
        "Variante de la fonction précédente pour le cas où une précision de 8 bits est choisie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JSLUU_H4FCoN"
      },
      "outputs": [],
      "source": [
        "# Renvoie le support de la matrice\n",
        "def compute_support_int(m):\n",
        "    N = m.shape[1]\n",
        "    return csr.sum(m/255)/(N*((N-1)/2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWhLEkhrFCoO"
      },
      "source": [
        "## Validation des candidats combinés\n",
        "\n",
        "Fonction qui retourne la combinaison des itemsets passée en argument si cette combinaison est viable compte tenu des itemsets fréquents de l'itération précédente, évaluant si :\n",
        "\n",
        "* Ils ont au moins k-2 éléments en commun\n",
        "* Tous ses sous-ensembles d'itemset incrémentiels de taille k-1 étaient fréquents dans l'itération ou la phase précédente.\n",
        "\n",
        "La condition si elles ont déjà été générées est évaluée après cette fonction avec un distinct() de la liste obtenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p-Cz-nwqFCoQ"
      },
      "outputs": [],
      "source": [
        "def combinedCandidates(itemset_pair, previous_frequent_itemsets):\n",
        "    #previous_frequent_itemsets = [set(i) for i in previous_frequent_itemsetsk]\n",
        "    result = ()\n",
        "    difference = set(itemset_pair[1])-set(itemset_pair[0])\n",
        "    if(len(difference) == 1):\n",
        "        result =  itemset_pair[0] + tuple(difference)\n",
        "        combinations = [x for x in itertools.combinations(result, len(result)-1)]\n",
        "        if not all(i in previous_frequent_itemsets for i in combinations):\n",
        "            result = ()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQ2YI48FCoR"
      },
      "source": [
        "## Combinaison des matrices\n",
        "Fonction qui reçoit en argument un itemset combiné et la liste des itemsets fréquents de l'itération précédente, avec leurs matrices respectives. Renvoie l'itemset et la matrice résultant de la combinaison des matrices de deux des itemsets de taille k-1 qui le composent, nous laissant avec le minimum de chacun de ses éléments (norme T de Gödel des matrices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xtWu0AZ4FCoT"
      },
      "outputs": [],
      "source": [
        "def ComputeCombinedMatrix(itemset, previous_frequent):\n",
        "    \n",
        "    itemset1 = itemset[:-1]\n",
        "    itemset2 = itemset[1:]\n",
        "    #itemset2 = itemset[:-2]+itemset[-1]\n",
        "    \n",
        "    \n",
        "    matriz_resultante = csr.minimum(previous_frequent.value[itemset1], previous_frequent.value[itemset2])\n",
        "    itemset_resultante = itemset1 + tuple(set(itemset2)-set(itemset1))\n",
        "    \n",
        "    return (itemset_resultante, matriz_resultante)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYr3rxifFCoV"
      },
      "source": [
        "## Function principale de l'algorithme.\n",
        "Renvoie la liste de tous les ensembles d'éléments fréquents dans l'ensemble de données donné, qui dépassent un certain support minimum. Il reçoit en entrée (arguments) le SparkContext, le jeu de données (distribué dans le cluster), le support minimum, un r pour effectuer les appels pour calculer le degré de corrélation entre les attributs. (Utilise une précision de 16 bits pour stocker les degrés d'accord)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UkqmdsvLFCoW"
      },
      "outputs": [],
      "source": [
        "def extractFrequentItemsets(sc, distDataset, min_supp, r):\n",
        "    \n",
        "    # Premiere etape de l'alogrithme\n",
        "    candidates = []\n",
        "\n",
        "    n_att = len(distDataset.value.columns)\n",
        "\n",
        "    # Nous ajoutons aux candidats tous les itemsets possibles de taille 2 (sauf équivalents)\n",
        "    for i in range(n_att):\n",
        "        for j in range(i+1,n_att):\n",
        "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'>')]]\n",
        "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'<')]]\n",
        "\n",
        "    \n",
        "    # Nous évaluons chacun des candidats possibles de manière distribuée dans le cluster, ne laissant que\n",
        "    # les itemsets fréquents de taille 2 avec leur tableau associé.\n",
        "    frequent_itemsets_k = sc.parallelize(candidates).map(lambda x: generate_matrix(x, distDataset, r))\\\n",
        "                                                    .filter(lambda x: compute_support(x[1]) >= min_supp)\\\n",
        "                                                    .collect()\n",
        "    \n",
        "\n",
        "    # Seconde phase de l'algorithme\n",
        "\n",
        "    frequent_itemsets_list = []\n",
        "    # Nous répétons ce processus jusqu'à ce que la liste des itemsets fréquents générés lors de l'itération précédente soit vide.\n",
        "    while(frequent_itemsets_k):\n",
        "        # Nous créons une copie des itemsets fréquents de la phase ou de l'itération précédente et leurs matrices respectives\n",
        "        # sur chacun des nœuds du cluster.\n",
        "        previous_frequent_dict = sc.broadcast(dict(frequent_itemsets_k))\n",
        "        previous_frequent_itemsets = [item for item in previous_frequent_dict.value] # Lista con los itemsets frecuentes anterior\n",
        "        frequent_itemsets_list += previous_frequent_itemsets\n",
        "        candidates_combinations = [x for x in itertools.combinations(previous_frequent_itemsets, 2)]\n",
        "\n",
        "        # Nous évaluons les combinaisons possibles de candidats de manière distribuée dans le cluster, ne laissant que\n",
        "        # les viables dont le support de la matrice dépasse le support minimum.\n",
        "        frequent_itemsets_k = sc.parallelize(candidates_combinations)\\\n",
        "                             .map(lambda x: combinedCandidates(x,previous_frequent_itemsets))\\\n",
        "                             .filter(lambda x: x)\\\n",
        "                             .distinct()\\\n",
        "                             .map(lambda x: ComputeCombinedMatrix(x, previous_frequent_dict))\\\n",
        "                             .filter(lambda x: compute_support(x[1]) >= min_supp)\\\n",
        "                             .collect()\n",
        "\n",
        "    \n",
        "    # Nous supprimons le contexte actuel.\n",
        "    sc.stop()\n",
        "    return frequent_itemsets_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdkek95EFCoZ"
      },
      "source": [
        "## Fonction principale identique à ci-dessus mais pour une précision de 8 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "61yoAYPrFCob"
      },
      "outputs": [],
      "source": [
        "def extractFrequentItemsets_int(sc, distDataset, min_supp, r):\n",
        "    \n",
        "    # Premiere phase de l'algorithme\n",
        "    candidates = []\n",
        "\n",
        "    n_att = len(distDataset.value.columns)\n",
        "\n",
        "    # Nous ajoutons aux candidats tous les itemsets possibles de taille 2 (sauf équivalents)\n",
        "    for i in range(n_att):\n",
        "        for j in range(i+1,n_att):\n",
        "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'>')]]\n",
        "            candidates += [[(distDataset.value.columns[i],'>'),(distDataset.value.columns[j],'<')]]\n",
        "\n",
        "    # Nous évaluons chacun des candidats possibles de manière distribuée dans le cluster, ne laissant que\n",
        "    # les itemsets fréquents de taille 2 avec leur tableau associé.\n",
        "    frequent_itemsets_k = sc.parallelize(candidates).map(lambda x: generate_matrix_int(x, distDataset, r))\\\n",
        "                                                    .filter(lambda x: compute_support_int(x[1]) >= min_supp)\\\n",
        "                                                    .collect()\n",
        "\n",
        "    \n",
        "\n",
        "    # Seconde phase de l'algorithme\n",
        "\n",
        "    frequent_itemsets_list = []\n",
        "    # Nous répétons ce processus jusqu'à ce que la liste des itemsets fréquents générés lors de l'itération précédente soit vide.\n",
        "    while(frequent_itemsets_k):\n",
        "        # Nous créons une copie des itemsets fréquents de la phase ou de l'itération précédente et leurs matrices respectives\n",
        "        # dans chacun des nœuds du cluster.\n",
        "        previous_frequent_dict = sc.broadcast(dict(frequent_itemsets_k))\n",
        "        previous_frequent_itemsets = [item for item in previous_frequent_dict.value] # Lista con los itemsets frecuentes anterior\n",
        "        frequent_itemsets_list += previous_frequent_itemsets\n",
        "        candidates_combinations = [x for x in itertools.combinations(previous_frequent_itemsets, 2)]\n",
        "\n",
        "        # Nous évaluons les combinaisons possibles de candidats de manière distribuée dans le cluster, ne laissant que\n",
        "        # avec les viables dont la matrice dépasse le support minimum.\n",
        "        frequent_itemsets_k = sc.parallelize(candidates_combinations)\\\n",
        "                             .map(lambda x: combinedCandidates(x,previous_frequent_itemsets))\\\n",
        "                             .filter(lambda x: x)\\\n",
        "                             .distinct()\\\n",
        "                             .map(lambda x: ComputeCombinedMatrix(x, previous_frequent_dict))\\\n",
        "                             .filter(lambda x: compute_support_int(x[1]) >= min_supp)\\\n",
        "                             .collect()\n",
        "\n",
        "    # Eliminons le contexte actuel.\n",
        "    sc.stop()\n",
        "    return frequent_itemsets_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHE50COmFCod"
      },
      "source": [
        "## On initialise le SparkContext en précisant le nom de l'application et le nombre de cœurs (en cas d'exécution sur une machine locale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GmJ-LJ5FFCoe"
      },
      "outputs": [],
      "source": [
        "# Initialisation du contexte\n",
        "def createSparkContext():\n",
        "    sc_conf = spark.SparkConf()\n",
        "    sc_conf.setMaster(\"local[*]\")    # Ici, nous pouvons spécifier le nombre de cœurs de processeur que nous voulons utiliser localement\n",
        "    sc_conf.setAppName(\"pattern mining\")\n",
        "    #sc_conf.set('spark.executor.instances', '2')\n",
        "    #sc_conf.set('spark.executor.memory', '2g')\n",
        "    #sc_conf.set('spark.executor.cores', '1')\n",
        "    #sc_conf.set('spark.cores.max', '1')\n",
        "    #sc_conf.set('spark.logConf', True)\n",
        "\n",
        "    sc = spark.SparkContext(conf=sc_conf)\n",
        "\n",
        "    return sc \n",
        "\n",
        "sc = createSparkContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donnons à Google Collab l'accès aux fichiers du drive, le jeu de données en fait"
      ],
      "metadata": {
        "id": "Sj7kx2na_act"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuiwgz8C_ZqS",
        "outputId": "86b7c2a6-fe99-41b0-d7fb-221747ec23f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "%cd MyDrive/TP - DATAMINING - M1 - GRITE\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjAGuLD6-t0h",
        "outputId": "7789b984-6049-4aca-e97a-f76257a97ef7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TP - DATAMINING - M1 - GRITE\n",
            "'Devoir Datamining - Graduels Patterns Mining.gdoc'   small.csv\n",
            " ImplementacionSparkNotebook.ipynb                    winequality-red.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2k-KT8FCof"
      },
      "source": [
        "## Exécution directe, en attribuant ici les valeurs que l'on veut aux variables\n",
        "\n",
        "Dans ImplementacionSpark.py (FICHIER SUR LE GITHUB), nous pouvons exécuter et attribuer des paramètres à partir de la ligne de commande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxVNV0OYFCog",
        "outputId": "54156681-0766-4547-a806-6e4a147d0e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temps d'execution de l'algorithme:  9.186166048049927\n",
            "\n",
            "\n",
            "Itemsets graduels frequents obtenus avec le support: 0.3\n",
            "\n",
            "\n",
            "(('fixed acidity', '>'), ('volatile acidity', '<')) \n",
            "\n",
            "(('fixed acidity', '>'), ('citric acid', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('free sulfur dioxide', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('density', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('pH', '<')) \n",
            "\n",
            "(('fixed acidity', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('alcohol', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('citric acid', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('free sulfur dioxide', '>')) \n",
            "\n",
            "(('volatile acidity', '>'), ('free sulfur dioxide', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('volatile acidity', '>'), ('total sulfur dioxide', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('density', '>')) \n",
            "\n",
            "(('volatile acidity', '>'), ('density', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('pH', '>')) \n",
            "\n",
            "(('volatile acidity', '>'), ('sulphates', '<')) \n",
            "\n",
            "(('volatile acidity', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('chlorides', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('free sulfur dioxide', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('density', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('density', '<')) \n",
            "\n",
            "(('citric acid', '>'), ('pH', '<')) \n",
            "\n",
            "(('citric acid', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('alcohol', '<')) \n",
            "\n",
            "(('residual sugar', '>'), ('free sulfur dioxide', '>')) \n",
            "\n",
            "(('residual sugar', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('residual sugar', '>'), ('density', '>')) \n",
            "\n",
            "(('residual sugar', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('chlorides', '>'), ('free sulfur dioxide', '>')) \n",
            "\n",
            "(('chlorides', '>'), ('pH', '<')) \n",
            "\n",
            "(('chlorides', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('density', '>')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('pH', '<')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('total sulfur dioxide', '>'), ('density', '>')) \n",
            "\n",
            "(('total sulfur dioxide', '>'), ('pH', '<')) \n",
            "\n",
            "(('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('total sulfur dioxide', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('total sulfur dioxide', '>'), ('alcohol', '<')) \n",
            "\n",
            "(('density', '>'), ('pH', '>')) \n",
            "\n",
            "(('density', '>'), ('pH', '<')) \n",
            "\n",
            "(('density', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('density', '>'), ('alcohol', '<')) \n",
            "\n",
            "(('pH', '>'), ('sulphates', '<')) \n",
            "\n",
            "(('pH', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('sulphates', '>'), ('alcohol', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('citric acid', '>'), ('total sulfur dioxide', '>'), ('pH', '<')) \n",
            "\n",
            "(('citric acid', '>'), ('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
            "\n",
            "(('fixed acidity', '>'), ('citric acid', '>'), ('pH', '<')) \n",
            "\n",
            "(('fixed acidity', '>'), ('density', '>'), ('pH', '<')) \n",
            "\n",
            "(('citric acid', '>'), ('free sulfur dioxide', '>'), ('total sulfur dioxide', '>')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('density', '>')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('pH', '<')) \n",
            "\n",
            "(('free sulfur dioxide', '>'), ('total sulfur dioxide', '>'), ('sulphates', '>')) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Execution\n",
        "r = 0.098\n",
        "\n",
        "min_supp = 0.3\n",
        "\n",
        "dataset_file = \"winequality-red.csv\"\n",
        "\n",
        "precision = '16bits'\n",
        "\n",
        "n_att = 12\n",
        "\n",
        "n_trans = 100\n",
        "\n",
        "sep = ';'\n",
        "\n",
        "if(dataset_file == 'pd_speech_features.csv'):\n",
        "    df = pd.read_csv(\"pd_speech_features.csv\",sep = \",\",skiprows = 1)   # Nous l'avons mis de côté car il a une ligne supplémentaire et deux colonnes que nous devons supprimer\n",
        "    my_data = df.iloc[:n_trans,2:n_att+2]\n",
        "else:\n",
        "    # Nous chargeons les données dans une dataframe pandas.\n",
        "    #df = pd.read_csv(\"seizure.csv\",sep = \",\")\n",
        "    df = pd.read_csv(dataset_file,sep = sep)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Nous conservons un sous-ensemble total des transactions et des attributs en fonction de la capacité de notre matériel\n",
        "#my_data = df.iloc[:80,1:-149]\n",
        "my_data = df.iloc[:50,:]\n",
        "#my_data = df.iloc[:100,1:-743]\n",
        "\n",
        "\n",
        "# Normalisation min max des donnees\n",
        "normalized_data = (my_data - my_data.min())/(my_data.max() - my_data.min())\n",
        "\n",
        "datadist = sc.broadcast(normalized_data)\n",
        "\n",
        "if precision == '8bits':\n",
        "        t1 = time.time()\n",
        "    \n",
        "        result = extractFrequentItemsets_int(sc,datadist, min_supp,r)\n",
        "    \n",
        "        t2 = time.time()\n",
        "else:  \n",
        "        t1 = time.time()\n",
        "        \n",
        "        result = extractFrequentItemsets(sc,datadist, min_supp,r)\n",
        "        \n",
        "        t2 = time.time()\n",
        "\n",
        "print(\"Temps d'execution de l'algorithme: \", t2-t1)\n",
        "print(\"\\n\\nItemsets graduels frequents obtenus avec le support: \" + str(min_supp) + \"\\n\\n\")\n",
        "for i in result:\n",
        "        print(i,\"\\n\")\n",
        "# Il est nécessaire de réinitialiser le SparkContext si nous voulons exécuter cette cellule plus d'une fois."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}